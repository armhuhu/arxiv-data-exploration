{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import feather\n",
    "from operator import itemgetter\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_papers = pd.read_pickle(\"dblp.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lecture Notes in Computer Science',\n",
       " 'international conference on acoustics, speech, and signal processing',\n",
       " 'international conference on robotics and automation',\n",
       " 'international conference on image processing',\n",
       " 'international conference on communications',\n",
       " 'international symposium on circuits and systems',\n",
       " 'global communications conference',\n",
       " 'international geoscience and remote sensing symposium',\n",
       " 'intelligent robots and systems',\n",
       " 'conference of the international speech communication association']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10_venues = ml_papers.venue.value_counts().index.tolist()[1:11]\n",
    "top_10_venues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_papers = ml_papers[ml_papers.venue.isin(top_10_venues)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>id</th>\n",
       "      <th>n_citation</th>\n",
       "      <th>references</th>\n",
       "      <th>title</th>\n",
       "      <th>venue</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[Milos Zelezný, Petr Císar, Zdenek Krnoul, Jan...</td>\n",
       "      <td>00e02aeb-b424-4ca8-b3ca-6e18e322f79e</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Design of an audio-visual speech corpus for th...</td>\n",
       "      <td>conference of the international speech communi...</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>[Claudius Gläser, Martin Heckmann, Frank Joubl...</td>\n",
       "      <td>02e8e38f-ed5c-43a4-b3a4-c5064a725a2d</td>\n",
       "      <td>2</td>\n",
       "      <td>[18b17dbd-4f51-411e-a099-efadf521f0d8, 24c7948...</td>\n",
       "      <td>Auditory-based formant estimation in noise usi...</td>\n",
       "      <td>conference of the international speech communi...</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>[Thomas Portele, Silke Goronzy, Martin Emele, ...</td>\n",
       "      <td>0564b85b-eac4-4a59-951e-3d8badc0c3e7</td>\n",
       "      <td>50</td>\n",
       "      <td>[6bb66751-ba4a-4740-9bc6-a41500f33022, a89dda2...</td>\n",
       "      <td>Smartkom-home - an advanced multi-modal interf...</td>\n",
       "      <td>conference of the international speech communi...</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>[Guo-Hong Ding, Yifei Zhu, Chengrong Li, Bo Xu]</td>\n",
       "      <td>0602535c-1d37-4e96-9882-9be45ecd334a</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Implementing vocal tract length normalization ...</td>\n",
       "      <td>conference of the international speech communi...</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>[Mireia Farrús, Michael Wagner, Jan Anguita, J...</td>\n",
       "      <td>0a11cd87-659d-45b8-b140-d8a8ff0974ee</td>\n",
       "      <td>50</td>\n",
       "      <td>[23af12d9-481d-459a-a1e2-23390f7b3c9a, 4047558...</td>\n",
       "      <td>Robustness of prosodic features to voice imita...</td>\n",
       "      <td>conference of the international speech communi...</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               authors  \\\n",
       "17   [Milos Zelezný, Petr Císar, Zdenek Krnoul, Jan...   \n",
       "54   [Claudius Gläser, Martin Heckmann, Frank Joubl...   \n",
       "83   [Thomas Portele, Silke Goronzy, Martin Emele, ...   \n",
       "90     [Guo-Hong Ding, Yifei Zhu, Chengrong Li, Bo Xu]   \n",
       "145  [Mireia Farrús, Michael Wagner, Jan Anguita, J...   \n",
       "\n",
       "                                       id  n_citation  \\\n",
       "17   00e02aeb-b424-4ca8-b3ca-6e18e322f79e           7   \n",
       "54   02e8e38f-ed5c-43a4-b3a4-c5064a725a2d           2   \n",
       "83   0564b85b-eac4-4a59-951e-3d8badc0c3e7          50   \n",
       "90   0602535c-1d37-4e96-9882-9be45ecd334a          14   \n",
       "145  0a11cd87-659d-45b8-b140-d8a8ff0974ee          50   \n",
       "\n",
       "                                            references  \\\n",
       "17                                                 NaN   \n",
       "54   [18b17dbd-4f51-411e-a099-efadf521f0d8, 24c7948...   \n",
       "83   [6bb66751-ba4a-4740-9bc6-a41500f33022, a89dda2...   \n",
       "90                                                 NaN   \n",
       "145  [23af12d9-481d-459a-a1e2-23390f7b3c9a, 4047558...   \n",
       "\n",
       "                                                 title  \\\n",
       "17   Design of an audio-visual speech corpus for th...   \n",
       "54   Auditory-based formant estimation in noise usi...   \n",
       "83   Smartkom-home - an advanced multi-modal interf...   \n",
       "90   Implementing vocal tract length normalization ...   \n",
       "145  Robustness of prosodic features to voice imita...   \n",
       "\n",
       "                                                 venue  year  \n",
       "17   conference of the international speech communi...  2002  \n",
       "54   conference of the international speech communi...  2008  \n",
       "83   conference of the international speech communi...  2003  \n",
       "90   conference of the international speech communi...  2002  \n",
       "145  conference of the international speech communi...  2008  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_papers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = ml_papers[['id', 'title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00e02aeb-b424-4ca8-b3ca-6e18e322f79e</td>\n",
       "      <td>Design of an audio-visual speech corpus for th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>02e8e38f-ed5c-43a4-b3a4-c5064a725a2d</td>\n",
       "      <td>Auditory-based formant estimation in noise usi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0564b85b-eac4-4a59-951e-3d8badc0c3e7</td>\n",
       "      <td>Smartkom-home - an advanced multi-modal interf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0602535c-1d37-4e96-9882-9be45ecd334a</td>\n",
       "      <td>Implementing vocal tract length normalization ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0a11cd87-659d-45b8-b140-d8a8ff0974ee</td>\n",
       "      <td>Robustness of prosodic features to voice imita...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       id  \\\n",
       "17   00e02aeb-b424-4ca8-b3ca-6e18e322f79e   \n",
       "54   02e8e38f-ed5c-43a4-b3a4-c5064a725a2d   \n",
       "83   0564b85b-eac4-4a59-951e-3d8badc0c3e7   \n",
       "90   0602535c-1d37-4e96-9882-9be45ecd334a   \n",
       "145  0a11cd87-659d-45b8-b140-d8a8ff0974ee   \n",
       "\n",
       "                                                 title  \n",
       "17   Design of an audio-visual speech corpus for th...  \n",
       "54   Auditory-based formant estimation in noise usi...  \n",
       "83   Smartkom-home - an advanced multi-modal interf...  \n",
       "90   Implementing vocal tract length normalization ...  \n",
       "145  Robustness of prosodic features to voice imita...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/tniyomkarn/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "np.random.seed(3)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tniyomkarn/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "documents['index'] = documents.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17     [design, audio, visual, speech, corpus, czech,...\n",
       "54     [auditori, base, formant, estim, nois, probabi...\n",
       "83     [smartkom, home, advanc, multi, modal, interfa...\n",
       "90     [implement, vocal, tract, length, normal, mllr...\n",
       "145                 [robust, prosod, featur, voic, imit]\n",
       "148             [text, speech, convers, applic, swedish]\n",
       "152    [phonem, recognit, combin, bayesian, linear, d...\n",
       "160    [languag, develop, extrem, childhood, depriv, ...\n",
       "175    [harmon, filter, joint, estim, pitch, voic, so...\n",
       "181      [high, perform, digit, recognit, real, environ]\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = documents['title'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 audio\n",
      "1 corpus\n",
      "2 czech\n",
      "3 design\n",
      "4 speech\n",
      "5 synthesi\n",
      "6 visual\n",
      "7 auditori\n",
      "8 base\n",
      "9 estim\n",
      "10 formant\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, 1), (67, 1), (89, 1), (105, 1), (288, 1), (291, 1), (627, 1)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 8 (\"base\") appears 1 time.\n",
      "Word 67 (\"japanes\") appears 1 time.\n",
      "Word 89 (\"dynam\") appears 1 time.\n",
      "Word 105 (\"generat\") appears 1 time.\n",
      "Word 288 (\"error\") appears 1 time.\n",
      "Word 291 (\"predict\") appears 1 time.\n",
      "Word 627 (\"question\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_4310 = bow_corpus[4310]\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                               dictionary[bow_doc_4310[i][0]], \n",
    "bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.535105759434703),\n",
      " (1, 0.34467077660903883),\n",
      " (2, 0.4422136096566205),\n",
      " (3, 0.17580047941595725),\n",
      " (4, 0.3311877422331826),\n",
      " (5, 0.24812578847444544),\n",
      " (6, 0.4440196722348407)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=5, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.025*\"channel\" + 0.021*\"cod\" + 0.019*\"time\" + 0.016*\"system\" + 0.016*\"filter\" + 0.016*\"estim\" + 0.014*\"base\" + 0.013*\"signal\" + 0.013*\"mimo\" + 0.012*\"algorithm\" + 0.011*\"frequenc\" + 0.010*\"adapt\" + 0.009*\"rate\" + 0.009*\"optim\" + 0.009*\"linear\" + 0.008*\"analysi\" + 0.008*\"error\" + 0.008*\"video\" + 0.008*\"complex\" + 0.008*\"design\"\n",
      "Topic: 1 \n",
      "Words: 0.044*\"robot\" + 0.020*\"base\" + 0.018*\"model\" + 0.016*\"control\" + 0.012*\"mobil\" + 0.011*\"design\" + 0.010*\"manipul\" + 0.010*\"human\" + 0.010*\"system\" + 0.009*\"environ\" + 0.009*\"dynam\" + 0.009*\"applic\" + 0.008*\"simul\" + 0.008*\"interact\" + 0.008*\"agent\" + 0.007*\"develop\" + 0.007*\"program\" + 0.007*\"approach\" + 0.007*\"learn\" + 0.007*\"remot\"\n",
      "Topic: 2 \n",
      "Words: 0.048*\"imag\" + 0.031*\"base\" + 0.018*\"data\" + 0.017*\"detect\" + 0.016*\"motion\" + 0.012*\"object\" + 0.012*\"estim\" + 0.010*\"model\" + 0.009*\"time\" + 0.009*\"method\" + 0.008*\"algorithm\" + 0.008*\"track\" + 0.008*\"segment\" + 0.008*\"scale\" + 0.007*\"sens\" + 0.007*\"compress\" + 0.007*\"multi\" + 0.007*\"local\" + 0.007*\"real\" + 0.007*\"surfac\"\n",
      "Topic: 3 \n",
      "Words: 0.067*\"network\" + 0.020*\"control\" + 0.020*\"wireless\" + 0.018*\"base\" + 0.013*\"sensor\" + 0.012*\"optim\" + 0.012*\"power\" + 0.011*\"multi\" + 0.011*\"effici\" + 0.010*\"energi\" + 0.009*\"distribut\" + 0.009*\"mobil\" + 0.008*\"cooper\" + 0.008*\"secur\" + 0.008*\"scheme\" + 0.008*\"perform\" + 0.008*\"algorithm\" + 0.008*\"communic\" + 0.007*\"system\" + 0.007*\"dynam\"\n",
      "Topic: 4 \n",
      "Words: 0.032*\"speech\" + 0.031*\"model\" + 0.030*\"base\" + 0.025*\"recognit\" + 0.014*\"featur\" + 0.014*\"learn\" + 0.011*\"languag\" + 0.011*\"detect\" + 0.011*\"speaker\" + 0.010*\"robust\" + 0.009*\"automat\" + 0.009*\"analysi\" + 0.008*\"classif\" + 0.008*\"acoust\" + 0.007*\"neural\" + 0.007*\"synthesi\" + 0.007*\"data\" + 0.006*\"improv\" + 0.006*\"cluster\" + 0.006*\"adapt\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1, 20):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dict = {\n",
    "    0: \"Communications\",\n",
    "    1: \"Robotics and Automation\",\n",
    "    2: \"Image Processing\",\n",
    "    3: \"Circuits and Systems\",\n",
    "    4: \"Signal and Speech Processing\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_prob = lda_model.get_document_topics(bow_corpus, per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for doc_topics, word_topics, phi_values in topics:\n",
    "#     print('New Document \\n')\n",
    "#     print ('Document topics:', doc_topics)\n",
    "# #     print 'Word topics:', word_topics\n",
    "# #     print 'Phi values:', phi_values\n",
    "#     print(\" \")\n",
    "#     print('-------------- \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [max(x[0],key=itemgetter(1))[0]  for x in topic_prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = np.array(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert authors and references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_papers['topic'] = topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_papers.authors = ml_papers.authors.apply(', '.join)\n",
    "ml_papers.loc[ml_papers['references'].isnull(),['references']] = ml_papers.loc[ml_papers['references'].isnull(),'references'].apply(lambda x: [])\n",
    "ml_papers.references = ml_papers.references.apply(', '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_papers = ml_papers.replace({\"topic\": topic_dict})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>id</th>\n",
       "      <th>n_citation</th>\n",
       "      <th>references</th>\n",
       "      <th>title</th>\n",
       "      <th>venue</th>\n",
       "      <th>year</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Milos Zelezný, Petr Císar, Zdenek Krnoul, Jan ...</td>\n",
       "      <td>00e02aeb-b424-4ca8-b3ca-6e18e322f79e</td>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "      <td>Design of an audio-visual speech corpus for th...</td>\n",
       "      <td>conference of the international speech communi...</td>\n",
       "      <td>2002</td>\n",
       "      <td>Signal and Speech Processing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Claudius Gläser, Martin Heckmann, Frank Joubli...</td>\n",
       "      <td>02e8e38f-ed5c-43a4-b3a4-c5064a725a2d</td>\n",
       "      <td>2</td>\n",
       "      <td>18b17dbd-4f51-411e-a099-efadf521f0d8, 24c79482...</td>\n",
       "      <td>Auditory-based formant estimation in noise usi...</td>\n",
       "      <td>conference of the international speech communi...</td>\n",
       "      <td>2008</td>\n",
       "      <td>Signal and Speech Processing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Thomas Portele, Silke Goronzy, Martin Emele, A...</td>\n",
       "      <td>0564b85b-eac4-4a59-951e-3d8badc0c3e7</td>\n",
       "      <td>50</td>\n",
       "      <td>6bb66751-ba4a-4740-9bc6-a41500f33022, a89dda25...</td>\n",
       "      <td>Smartkom-home - an advanced multi-modal interf...</td>\n",
       "      <td>conference of the international speech communi...</td>\n",
       "      <td>2003</td>\n",
       "      <td>Robotics and Automation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Guo-Hong Ding, Yifei Zhu, Chengrong Li, Bo Xu</td>\n",
       "      <td>0602535c-1d37-4e96-9882-9be45ecd334a</td>\n",
       "      <td>14</td>\n",
       "      <td></td>\n",
       "      <td>Implementing vocal tract length normalization ...</td>\n",
       "      <td>conference of the international speech communi...</td>\n",
       "      <td>2002</td>\n",
       "      <td>Signal and Speech Processing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Mireia Farrús, Michael Wagner, Jan Anguita, Ja...</td>\n",
       "      <td>0a11cd87-659d-45b8-b140-d8a8ff0974ee</td>\n",
       "      <td>50</td>\n",
       "      <td>23af12d9-481d-459a-a1e2-23390f7b3c9a, 40475584...</td>\n",
       "      <td>Robustness of prosodic features to voice imita...</td>\n",
       "      <td>conference of the international speech communi...</td>\n",
       "      <td>2008</td>\n",
       "      <td>Signal and Speech Processing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               authors  \\\n",
       "17   Milos Zelezný, Petr Císar, Zdenek Krnoul, Jan ...   \n",
       "54   Claudius Gläser, Martin Heckmann, Frank Joubli...   \n",
       "83   Thomas Portele, Silke Goronzy, Martin Emele, A...   \n",
       "90       Guo-Hong Ding, Yifei Zhu, Chengrong Li, Bo Xu   \n",
       "145  Mireia Farrús, Michael Wagner, Jan Anguita, Ja...   \n",
       "\n",
       "                                       id  n_citation  \\\n",
       "17   00e02aeb-b424-4ca8-b3ca-6e18e322f79e           7   \n",
       "54   02e8e38f-ed5c-43a4-b3a4-c5064a725a2d           2   \n",
       "83   0564b85b-eac4-4a59-951e-3d8badc0c3e7          50   \n",
       "90   0602535c-1d37-4e96-9882-9be45ecd334a          14   \n",
       "145  0a11cd87-659d-45b8-b140-d8a8ff0974ee          50   \n",
       "\n",
       "                                            references  \\\n",
       "17                                                       \n",
       "54   18b17dbd-4f51-411e-a099-efadf521f0d8, 24c79482...   \n",
       "83   6bb66751-ba4a-4740-9bc6-a41500f33022, a89dda25...   \n",
       "90                                                       \n",
       "145  23af12d9-481d-459a-a1e2-23390f7b3c9a, 40475584...   \n",
       "\n",
       "                                                 title  \\\n",
       "17   Design of an audio-visual speech corpus for th...   \n",
       "54   Auditory-based formant estimation in noise usi...   \n",
       "83   Smartkom-home - an advanced multi-modal interf...   \n",
       "90   Implementing vocal tract length normalization ...   \n",
       "145  Robustness of prosodic features to voice imita...   \n",
       "\n",
       "                                                 venue  year  \\\n",
       "17   conference of the international speech communi...  2002   \n",
       "54   conference of the international speech communi...  2008   \n",
       "83   conference of the international speech communi...  2003   \n",
       "90   conference of the international speech communi...  2002   \n",
       "145  conference of the international speech communi...  2008   \n",
       "\n",
       "                            topic  \n",
       "17   Signal and Speech Processing  \n",
       "54   Signal and Speech Processing  \n",
       "83        Robotics and Automation  \n",
       "90   Signal and Speech Processing  \n",
       "145  Signal and Speech Processing  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_papers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_papers.reset_index(drop=True).to_feather('ml_papers.feather')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
